% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/miscellaneous.R
\name{gradParallel}
\alias{gradParallel}
\title{Parallelised gradient computation}
\usage{
gradParallel(
  func,
  x,
  side = c("central", "forward", "backward"),
  order = 2,
  h = if (side[1] == "central") .Machine$double.eps^(1/3) else sqrt(.Machine$double.eps),
  parallel = FALSE,
  use.mclapply = FALSE,
  cores = 2,
  cluster = NULL,
  load.balance = TRUE,
  ...
)
}
\arguments{
\item{func}{A function that returns a numeric scalar or a vector. If the function is vector-valued, the, the result is the Jacobian.}

\item{x}{A point at which the gradient or Jacobian needs to be estimated.}

\item{side}{Passed to \code{fdCoef()}. Centred or one-sided differences. Unless the function is computationally prohibitively expensive, two-sided differences are strongly recommended.}

\item{order}{Desired order of accuracy. The error is usually O(h^order). To achieve this order of accuracy, the function needs to be evaluated \code{order*length(x)} times.}

\item{h}{The numerical difference step size. Too large = the slope of the secant is a bad estimator of the gradient, too small = ill conditioning (0/0).}

\item{parallel}{If TRUE, estimates the gradient via finite differences where the function is evaluated in parallel.}

\item{use.mclapply}{If TRUE, uses a much quicker and simpler \code{parallel::mclapply} an process forking; if FALSE, requires a properly set up cluster.}

\item{cores}{Number of forked processes.}

\item{cluster}{A cluster on which the computations are done.}

\item{load.balance}{If TRUE, disables pre-scheduling for \code{mclapply} or enables load balancing via \code{parLapplyLB}.}

\item{...}{Passed to `func`.

Note that for one-sided problems, the step size that make the formula error
equal to the truncation error is of the order Mach.eps^(1/2) and for two-sided, Mach.eps^(1/3).
However, the optimal step size depends on the value of the higher-order derivatives
that is not available in general (or required extra computation that is, in turn, prone to numerical error).}
}
\value{
If \code{func} returns a scalar, a vector of the same length as \code{x}.
If \code{func} returns a vector, then, a matrix of dimensions \code{length(f(x)) length(x)}
}
\description{
Computes a two- or one-sided numerical derivative that approximates the gradient | Jacobian using the indicated number of cores for maximum efficiency.
}
\examples{
\dontrun{
slowFunScalar <- function(x) {Sys.sleep(0.04); print(x, digits = 12); sum(sin(x))}
slowFunVector <- function(x) {Sys.sleep(0.04); print(x, digits = 12); c(sum(sin(x)), sum(exp(x)))}
true.g <- cos(1:4) # Analytical gradient
true.j <- rbind(cos(1:4), exp(1:4)) # Analytical Jacobian
system.time(g.slow <- numDeriv::grad(slowFunScalar, x = 1:4) - true.g)
system.time(j.slow <- numDeriv::jacobian(slowFunVector, x = 1:4) - true.j)
system.time(g.fast <- gradParallel(slowFunScalar, x = 1:4,
                                   parallel = TRUE, use.mclapply = TRUE, cores = 4) - true.g)
system.time(j.fast <- gradParallel(slowFunVector, x = 1:4,
                                   parallel = TRUE, use.mclapply = TRUE, cores = 4) - true.j)
system.time(j.fast4 <- gradParallel(slowFunVector, x = 1:4, order = 4,
                                    parallel = TRUE, use.mclapply = TRUE, cores = 4) - true.j)
rownames(j.slow) <- c("numDeriv.jacobian", "")
rownames(j.fast) <- c("fast.jacobian.order2", "")
rownames(j.fast4) <- c("fast.jacobian.order4", "")
# Discrepancy
rbind(numDeriv.grad = g.slow, fast.grad = g.fast, j.slow, j.fast, j.fast4)
# The order-4 derivative is more accurate for functions with large high-order derivatives
}

}

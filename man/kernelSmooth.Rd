% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/smoothers.R
\name{kernelSmooth}
\alias{kernelSmooth}
\title{Local kernel smoother}
\usage{
kernelSmooth(
  x,
  y,
  xgrid = NULL,
  bw = NULL,
  kernel = c("gaussian", "uniform", "triangular", "epanechnikov"),
  order = 2,
  convolution = FALSE,
  PIT = FALSE,
  LOO = FALSE,
  degree = 0,
  trim = function(x) 0.01/length(x),
  robust.iterations = 0,
  return.grid = FALSE
)
}
\arguments{
\item{x}{A numeric vector, matrix, or data frame containing observations. For density, the
points used to compute the density. For kernel regression, the points corresponding to
explanatory variables.}

\item{y}{A numeric vector of dependent variable values.}

\item{xgrid}{A vector or a matrix of data points with \code{ncol(xgrid) = ncol(x)}
at which the user desires to compute the weights, density, or predictions.
In other words, this is the requested evaluation grid.
If \code{NULL}, then \code{x} itself is used as the grid.}

\item{bw}{Bandwidth for the kernel: a scalar or a vector of the same length as \code{ncol(x)}.
Since it is the crucial parameter in many applications, a warning is thrown if the bandwidth
is not supplied, and then, Silverman's rule of thumb (via \code{bw.row()}) is applied
to *every dimension* of \code{x}.}

\item{kernel}{Character describing the desired kernel type (Gaussian is infinitely smooth but does not provide finite support).}

\item{order}{An integer: 2, 4, or 6. Order-2 kernels are the standard kernels that
are positive everywhere. Orders 4 and 6 produce some negative values, which reduces bias but may hamper density estimation.}

\item{convolution}{Logical: if FALSE, returns the usual kernel. If TRUE, returns
the convolution kernel that is used in density cross-validation.}

\item{PIT}{If TRUE, the Probability Integral Transform (PIT) is applied to all columns
  of \code{x} via \code{ecdf} in order to map all values into the [0, 1] range. May
  be an integer vector of indices of columns to which the PIT should be applied.

Note that if \code{pit = TRUE}, then the kernel-based weights become nearest-neighbour weights (i.e. not much different from the ones used
internally in the built-in \code{loess} function) since the distances now depend on the ordering of data, not the values per se.}

\item{LOO}{Logical: If \code{TRUE}, the leave-one-out estimator is returned.}

\item{degree}{Integer: 0 for locally constant estimator (Nadaraya--Watson), 1 for
locally linear (Cleveland's LOESS), 2 for locally quadratic (use with care, less stable, requires larger bandwidths)}

\item{trim}{Trimming function for small weights to speed up locally weighted regression (if \code{degree} is 1 or 2).}

\item{robust.iterations}{The number of robustifying iterations (due to Cleveland, 1979). If greater than 0, \code{xgrid} is ignored.}

\item{return.grid}{If \code{TRUE}, prepends \code{xgrid} to the return results.

Standardisation is recommended for the purposes of numerical stability (sometimes
  \code{lm()} might choke when the dependent variable takes very large absolute
  values and its square is used).}
}
\value{
A vector of predicted values or, if \code{return.grid} is \code{TRUE},
  a matrix with the predicted values in the last column.
}
\description{
Local kernel smoother
}

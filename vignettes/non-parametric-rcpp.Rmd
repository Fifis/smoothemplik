---
title: "Using Rcpp to speed up non-parametric estimation in R"
output:
  rmarkdown::html_vignette:
    toc: true
author: "Andreï V. Kostyrka, University of Luxembourg"
date: Sys.date()
abstract: "This vignette demonstrates how to carry out non-parametric density and kernel estimation with a high degree of flexibiity and faster than via the straightforward textbook approach."
keywords: "non-parametric methods, density estimation, kernel smoothing, Nadaraya-Watson, local regression"
vignette: >
  %\VignetteIndexEntry{Using Rcpp to speed up non-parametric estimation in R}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::knit_hooks$set(pngquant = knitr::hook_pngquant)
knitr::opts_chunk$set(
  dev = "png",
  dev.args = list(type = "cairo-png"),
  fig.width = 640 / 72,
  fig.height = 480 / 72,
  dpi = 72,
  fig.retina = 1,
  collapse = TRUE,
  comment = "#>",
  pngquant = "--speed=1 --quality=50-60"
)
```

```{r setup}
library(smoothemplik)
```

## Kernel methods

The most straightforward approach to implement kernel weights, kernel density estimation, and locally constant kernel regression in **R** would be as follows.

The kernel weights are used in many applications to define a local neighbourhood for a point, i.e. the value of the kernel function:
\[
w_{ij} := k\left( \frac{X_i - X_j}{b} \right),
\]
where $k$ is a function that typically satisfies the following properties:

1. Symmetry: $k(-u) = k(u)$, $u\in \mathbb{R}$;
1. Unit integral over the reals:  $\int_{\mathbb{R}} k = 1$;
1. Second order: $\int_{\mathbb{R}} u^2 k(u) \in (0, \infty)$.


All of these properties can be relaxed. Asymmetrical kernels are sometimes used to improve the behaviour of KDEs when the support of the data is bounded. The integral taken over the real line being equal to one simplifies calculations and formulæ for densities owing to its built-in normalisation. Finally, higher-order kernels are often used to reduce bias at the cost of introducing negative values of $k(u)$, which is necessary in some applications (e.\,g.\ Robinson (1988) semi-parametric regression with more than 5 non-parametric regressors), but undesirable or even impossible in other ones (density estimation, local likelihood computation).

The kernel functions implemented in the `smoothemplik` package are:

* Uniform: $k(u) = \frac12 \mathbb{I}(|u|\le 1)$;
* Triangular: $k(u) = \frac12 \mathbb{I}(|u|\le 1)$;
* Epanechnikov: $k(u) = \frac34 (1-u^2) \mathbb{I}(|u| \le 1)$;
* Gaussian (default): $k = \varphi(u) := (2\pi)^{-1/2} \exp (-u^2/2)$.

The kernel weights $w_{ij}$ do not add up to unity; for most applications they can be used `as is' to represent the relative importance regardless of their total. However, for density and regression applications, re-scaled weights are necessary.

Multi-variate kernels are defined similarly. For simplicity, we restrict or scope to product kernels only and re-define the kernel weights:
\[
w_{ij} := \prod_{l=1}^d k\left( \frac{X_i^{(l)} - X_j^{(l)}}{b^{(l)}} \right) := K\left( \frac{X_i - X_j}{b} \right),
\]
where $d = \dim X$ is the dimension of the data; the dimension of $b$ is also $d$.

Computations could be sped up twice for certain applications through exploiting the symmetry of the kernel function; however, in most cases, the grid of evaluation points for non-parametric methods is pre-defined, yielding a non-square weight matrix.

Univariate kernel weights:
```{r}
# x = numeric vector, xgrid = numeric vector, bw = scalar, kfun = function
kernelWeights1R <- function(x, xgrid, bw, kfun) {
  kfun(outer(xgrid, x, "-") / bw)
}
```

Here, type / length checks are skipped because the goal is to show the `Rcpp` solution superiority, not sanitise the inputs for a benchmark function. The hidden function `smoothemplik:::.prepareKernel` does these checks before calling the C++ function.

```{r}
kernelWeightsR <- function(x, xgrid = NULL, bw = 1, kfun = stats::dnorm) {
  if (is.null(dim(x))) x <- as.matrix(x)
  if (is.null(xgrid)) xgrid <- x
  if (is.null(dim(xgrid))) xgrid <- as.matrix(xgrid)
  d <- ncol(x)
  if (d > 1 & length(bw) == 1) bw <- rep(bw, d)
  pk <- kernelWeights1R(x = x[, 1], xgrid = xgrid[, 1], bw = bw[1], kfun = kfun)
  if (d > 1) { # Accumulating the product kernel
    for (i in 2:d) {
      pk <- pk * kernelWeights1R(x = x[, i], xgrid = xgrid[, i], bw = bw[i], kfun = kfun)
    }
  }
  return(pk)
}
```

The Parzen--Rosenblatt density estimator a rescaled sum of the kernel functions:
\[
\hat f_X(x) := \frac{1}{nb^{(1)} \cdots b^{(d)}} \sum_{i=1}^n K\left( \frac{X_i - x}{b} \right)
\]
```{r}
kernelDensityR <- function(x, xgrid = NULL, bw = 1, kfun = stats::dnorm) {
  x1d <- is.null(dim(x))
  n <- if (x1d) length(x) else nrow(x)
  if (isTRUE(ncol(x) > 1) & length(bw) == 1) bw <- rep(bw, ncol(x))
  pk <- kernelWeightsR(x = x, xgrid = xgrid, bw = bw, kfun = kfun)
  return(rowSums(pk) / (n * prod(bw)))
}
```

The Nadaraya--Watson regression function estimator a local (weighted) average:
\[
\hat m(x) = \hat{\mathbb{E}}(Y \mid X = x) := \frac{\sum_{i=1}^n Y_i K((X_i - x)/b)}{\sum_{i=1}^n K((X_i - x)/b)}
\]

For optimal bandwidth calculation, quite often, a Leave-One-One (LOO) variety is used:
\[
\hat m_{-i}(x) := \frac{\sum_{j\ne i} Y_j K((X_j - x)/b)}{\sum_{j\ne i} K((X_j - x)/b)}
\]
It can be implemented by setting the weight of the $i$^th observation to zero. Note that LOO estimator makes practical sense only on a grid of original observed points, $\{X_i\}_{i=1}^n$, or its subset.

```{r}
kernelSmoothR <- function(x, y, xgrid = NULL, bw = 1,
                          kfun = stats::dnorm, LOO = FALSE) {
  pk <- kernelWeightsR(x = x, xgrid = xgrid, bw = bw, kfun = kfun)
  if (LOO) diag(pk) <- 0
  return(rowSums(sweep(pk, 2, y, "*")) / rowSums(pk))
}
```

### Speed-ups

1. The core of the package is built on `RcppArmadillo` for quick vector and matrix manipulations.
1. All kernel calculations are done in place to save memory; no extra vectors are initialised; the return vector is created at the beginning and gradually substituted based on the chosen kernel. E.\,g. `ax = abs(x)`, replacements are done for `ax`, like `if (ax[i] < 1) ax[i] = 0.75*(1 - ax[i]^2)`, and `ax` itself is returned at the end. The Nadaraya--Watson numerator is also computed in place from the weight matrix, thus saving memory by never using more than one large matrix.
1. Following Silverman (1986, 4.4), we perform the bandwidth scaling $\frac{X_i-x}{b} = \frac{X_i}{b}-\frac{x}{b} = X^*_i - x^*$ to reduce the number or arithmetic operation in the double loop. The scaling factors in the form of $\frac{1}{nb^{(1)} \cdots b^{(d)}}$ are also applied after the final vector or kernel sums is obtained.
1. Qualitatively different 4th-order kernels were developed to reduce the polynomial degree of the convolution kernels. If a kernel has polynomial degree $p$, is 4th-order analogue has degree $p$ as well, yielding convolution polynomial of degree at most $2p+1$. The traditional approach described in Li \& Racine (2007, section 1.11, p. 34) adds an extra polynomial; for the Epanechnikov kernel, their 4th-order version has degree 4, and convolution, degree $4\cdot 2 + 1 = 9$, whereas our version, consisting of piecewise quadratic polynomials, has degree 2 and convolution degree 5.

## Univariate kernel estimation

To test the speed gains, we benchmark these obvious solutions with their `Rcpp` counterparts implemented in this package. We draw $n=300$ realisations of the $X \sim\chi^2_3$ random variable, generate $Y := \sin X + U$, where $U\sim\mathcal{N}(0, 1)$ is the error. Define the grid to be 100 points uniformly spaced from 0 to 15. For simplicity, let the chosen bandwidth be $b = 0.4$, There are 3 objectives:
1. Generate a $100 \times 300$ matrix of kernel weights $w_{ij} = K((X_i - x_j)/b)$;
1. Estimate the density $f_X(x)$ on the grid;
1. Estimate the conditional expectation function $m(x) = \mathbb{E}(Y \mid X= x)$ on the grid.

```{r}
set.seed(1)
X <- sort(rchisq(300, 3))
xg <- seq(0, max(X), length.out = 100)
Y <- sin(X) + rnorm(100)
bw <- 0.5 # Bandwidth
```

Now, we pit the pure R and Rcpp functions:
```{r}
library(microbenchmark)
microbenchmark(
  wCPP <- kernelWeights(X, xgrid = xg, bw = bw),
  wR   <- kernelWeightsR(X, xgrid = xg, bw = bw),
  times = 20, unit = "ms", check = "equal"
)

microbenchmark(
  fCPP <- kernelDensity(X, xgrid = xg, bw = bw),
  fR   <- kernelDensityR(X, xgrid = xg, bw = bw),
  times = 20, unit = "ms", check = "equal"
)

microbenchmark(
  mCPP <- kernelSmooth(X, Y, xgrid = xg, bw = bw),
  mR   <- kernelSmoothR(X, Y, xgrid = xg, bw = bw),
  times = 20, unit = "ms", check = "equal"
)
```

Note that there are tiny discrepancies related to the order of operations and finite machine precision:
```{r}
all.equal(wR, wCPP, tolerance = 1e-16)
all.equal(fR, fCPP, tolerance = 1e-16)
all.equal(mR, mCPP, tolerance = 1e-16)
```

We can visualised the result and see that the discrepancies are tiny
```{r}
par(mfrow = c(2, 1), mar = c(4, 4, 2, 3))
plot(X, Y, bty = "n", main = "Non-parametric regression (black) and density (blue) estimate")
lines(xg, mCPP, lwd = 2)
par(new = TRUE)
plot(xg, fCPP, type = "l", lwd = 2, yaxt = "n", bty = "n", xaxt = "n", col = 4, xlab = "", ylab = "")
axis(4, col = 4, col.axis = 4)
plot(xg, fR - fCPP, bty = "n", ylab = "", xlab = "X",
     main = "Discrepancy between the R and C++ implementation")
```

Different kernel shapes can be used upon user request (all of them, except for the default Gaussian, have finite support on $[-1, 1]$):
```{r}
par(mfrow = c(1, 1))
fCPPunif <- kernelDensity(X, xgrid = xg, bw = bw * sqrt(3), kernel = "uniform")
fCPPtrng <- kernelDensity(X, xgrid = xg, bw = bw * sqrt(3), kernel = "triangular")
fCPPepan <- kernelDensity(X, xgrid = xg, bw = bw * sqrt(5), kernel = "epanechnikov")
plot(xg, fCPP, ylim = range(0, fCPP, fCPPunif, fCPPtrng, fCPPepan), xlab = "X",
     type = "l", bty = "n", main = "Various kernel shapes", ylab = "Density")
rug(X)
lines(xg, fCPPunif, col = 2)
lines(xg, fCPPtrng, col = 3)
lines(xg, fCPPepan, col = 4)
legend("topright", c("Gaussian", "Uniform", "Triangular", "Epanechnikov"), lwd = 1, col = 1:4)
```

The speed-up is more substantial with large data sets:
```{r}
ns <- c(500, 1000, 2000)
timings <- sapply(ns, function(n) {
  set.seed(1)
  X <- rchisq(n, 3)
  b <- bw.rot(X)
  a <- summary(microbenchmark(
    fR   <- kernelDensityR(X, bw = b),
    fCPP <- kernelDensity(X, bw = b),
    times = 3, unit = "ms", check = "equal", control = list(warmup = 1)
  ))
  c(R = a$median[1], CPP = a$median[2])
})
colnames(timings) <- paste0("n=", ns)
print(timings, 2)
```

## Multivariate kernel estimation

For visual clarity purposes, we generate the data according to a similar law with $\dim X = 2$, $n = 100$, $X \sim\chi^2_3$ (IID), $Y := \sin X^{(1)} + \sin X^{(2)} + U$, $U\sim\mathcal{N}(0, 1)$. Define the grid to be 50 points uniformly spaced from 0 to 13. For simplicity, let the chosen bandwidth be $b = (0.7, 0.8)$. Again, we generate kernek weights, estimate the density and the regression function.

```{r}
set.seed(1)
n <- 100
ng <- 30
X <- matrix(rchisq(n*2, 3), ncol = 2)
xg0 <- seq(0, 13, length.out = ng)
xg <- expand.grid(X1 = xg0, X2 = xg0)
Y <- sin(X[, 1]) + sin(X[, 2]) + rnorm(n)
bw <- c(0.7, 0.8)

microbenchmark(
  wCPP <- kernelWeights(X, xgrid = xg, bw = bw),
  wR <- kernelWeightsR(X, xgrid = xg, bw = bw),
  times = 10, unit = "ms", check = "equal")
all.equal(wR, wCPP, tolerance = 1e-16)
```

We see that C++ is roughly 4 times faster. In terms of accuracy, the difference is minuscule:
```{r}
max.dw <- apply(wR - wCPP, 1, function(x) max(abs(x), na.rm = TRUE))
filled.contour(xg0, xg0, log10(matrix(max.dw, ng, ng)), xlab = "X1", ylab = "X2",
               main = "Log10(discrepancy) between R and C++ kernel weights", asp = 1)
points(X[, 1], X[, 2], pch = 16)
```

We redo the same test for the kernel density estimator:
```{r}
microbenchmark(
  fCPP <- kernelDensity(X, xgrid = xg, bw = bw),
  fR <- kernelDensityR(X, xgrid = xg, bw = bw),
times = 5, unit = "ms", check = "equal")
all.equal(fR, fCPP, tolerance = 1e-16)
```

The difference between the two different kernels is better visible in 3D:
```{r}
fCPPunif <- kernelDensity(X, xgrid = xg, bw = bw * sqrt(3), kernel = "uniform")
fCPPtrng <- kernelDensity(X, xgrid = xg, bw = bw * sqrt(3), kernel = "triangular")
fCPPepan <- kernelDensity(X, xgrid = xg, bw = bw * sqrt(5), kernel = "epanechnikov")
par(mfrow = c(2, 2), mar = c(0.5, 0.5, 2, 0.5))
persp(xg0, xg0, matrix(fCPP, nrow = ng), theta = 120, phi = 20, main = "Gaussian", xlab = "X1", ylab = "X2", zlab = "Density")
persp(xg0, xg0, matrix(fCPPunif, nrow = ng), theta = 120, phi = 20, main = "Uniform", xlab = "X1", ylab = "X2", zlab = "Density")
persp(xg0, xg0, matrix(fCPPtrng, nrow = ng), theta = 120, phi = 20, main = "Triangular", xlab = "X1", ylab = "X2", zlab = "Density")
persp(xg0, xg0, matrix(fCPPepan, nrow = ng), theta = 120, phi = 20, main = "Epanechnikov", xlab = "X1", ylab = "X2", zlab = "Density")
```

Now, we run a quick benchmark to see how the problem scales with the sample size and the dimension of the data. These evaluations take longer, therefore, we can use the built-in `system.time` because the CPU overead is negligible:
```{r}
ns <- c(125, 500)
dims <- c(2, 6)
nd <- expand.grid(n = ns, dim = dims)

MVtimings <- t(sapply(1:nrow(nd), function(i) {
  set.seed(1)
  X <- matrix(rchisq(nd$n[i] * nd$dim[i], 3))
  b <- bw.rot(X)
  aR <- system.time(kernelDensityR(X, bw = b))["elapsed"]
  aCPP <- system.time(kernelDensity(X, bw = b))["elapsed"]
  c(R = aR, CPP = aCPP)
}))
MVtimings <- cbind(nd, MVtimings)
print(MVtimings, 2)
```

Finally, we can visualise the kernel regression estimator with various kernels and degrees, and robustness operations (described in the original Cleveland 1979 LOESS paper). Since the true functional dependence of $Y$ on $X$ is a sine wave, we expect the second-degree local estimator to perform the best.

Here, instead of choosing the bandwidth using the rule of thumb, we plot the cross-validation function (assuming that the bandwidth should be the same across all dimensions):
```{r}
bwgrid <- seq(0.2, 3, .1)
bws0 <- suppressWarnings(LSCV(X, Y, bw = bwgrid, degree = 0, same = TRUE))
bws1 <- suppressWarnings(LSCV(X, Y, bw = bwgrid, degree = 1, same = TRUE))
bws2 <- suppressWarnings(LSCV(X, Y, bw = bwgrid, degree = 2, same = TRUE))
bw.cv <- cbind(bws0, bws1, bws2)
bw.opt <- bwgrid[apply(bw.cv, 2, which.min)]
par(mar = c(4, 4, 0.5, 0.5))
matplot(bwgrid, bw.cv, lty = 1, type = "l", bty = "n", lwd = 2, col = 1:3,
        xlab = "Bandwidth", ylab = "Out-of-sample prediction error", log = "y")
legend("topright", paste("Degree", 0:2), lwd = 2, lty = 1, col = 1:3, bty = "n")
```

To avoid the low-density region, we focus on a smaller range of regressor values, $[0, 8]$:
```{r}
xg0 <- seq(0, 8, length.out = ng)
xg <- expand.grid(X1 = xg0, X2 = xg0)
yhat <- sapply(1:3, function(i) kernelSmooth(x = X,  y = Y, xgrid = xg, bw = bw.opt[i], degree = i-1))
par(mfrow = c(2, 2), mar = c(0.2, 0.2, 2, 0.2))
for (i in 1:3) {
  p <- persp(xg0, xg0, matrix(yhat[, i], ng, ng), ticktype = "detailed",
                     main = paste0("Degree ", i-1), theta = 30, phi = 25,
                     xlab = "X1", ylab = "X2", zlab = "Y", zlim = range(yhat, Y))
  points(trans3d(X[, 1], X[, 2], Y, p), col = 2)
} 
```

We can judge the fit quality by the percentage of explained variance (the issue of over-fitting with small bandwidths has been mitigated via cross-validation) using the available observations as the grid.

```{r}
ys <- sapply(1:3, function(i) kernelSmooth(x = X,  y = Y, bw = bw.opt[i], degree = i-1))
colnames(ys) <- paste("Degree", 0:2)
print(cor(cbind(Y, ys))[1, 2:4], 3)
```

